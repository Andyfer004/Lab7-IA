{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP951US1l5OVsi9Aov29sz6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Andyfer004/Lab7-IA/blob/main/Lab7IA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1 - Teoría\n",
        "\n",
        "## 1. ¿Qué es el temporal difference learning y en qué se diferencia de los métodos tradicionales de aprendizaje supervisado? Explique el concepto de \"error de diferencia temporal\" y su papel en los algoritmos de aprendizaje por refuerzo\n",
        "\n",
        "**Temporal Difference Learning (TD Learning)** es un método de aprendizaje por refuerzo que permite a un agente aprender una función de evaluación a partir de la experiencia sin necesidad de un modelo del entorno. A diferencia de los métodos tradicionales de aprendizaje supervisado, TD Learning no requiere datos etiquetados con salidas correctas, sino que ajusta su política basándose en las recompensas obtenidas.\n",
        "\n",
        "El **error de diferencia temporal (TD Error)** mide la diferencia entre la recompensa esperada y la obtenida en un estado determinado. Se define como:\n",
        "\n",
        "$$ TD\\_Error = R + \\gamma V(s') - V(s) $$\n",
        "\n",
        "Donde:\n",
        "- \\( R \\) es la recompensa obtenida.\n",
        "- \\( \\gamma \\) es el factor de descuento.\n",
        "- \\( V(s') \\) es el valor del estado siguiente.\n",
        "- \\( V(s) \\) es el valor del estado actual.\n",
        "\n",
        "El TD Error permite actualizar la función de evaluación \\( V(s) \\) de manera progresiva a medida que el agente obtiene más experiencia.\n",
        "\n",
        "\n",
        "\n",
        "## 2. En el contexto de los juegos simultáneos, ¿cómo toman decisiones los jugadores sin conocer las acciones de sus oponentes? De un ejemplo de un escenario del mundo real que pueda modelarse como un juego simultáneo y discuta las estrategias que los jugadores podrían emplear en tal situación\n",
        "\n",
        "En juegos simultáneos, los jugadores toman sus decisiones sin conocer las acciones del oponente. Para optimizar sus resultados, pueden emplear:\n",
        "\n",
        "- **Estrategia pura:** Siempre elegir la misma acción.\n",
        "- **Estrategia mixta:** Asignar probabilidades a cada acción para hacer la estrategia impredecible.\n",
        "\n",
        "### **Ejemplo: Negociaciones salariales**\n",
        "Dos empleados pueden negociar su sueldo sin conocer las ofertas de la empresa. Algunas estrategias que pueden seguir son:\n",
        "- **Estrategia agresiva:** Pedir un aumento alto esperando que la empresa acepte.\n",
        "- **Estrategia conservadora:** Pedir un aumento moderado para evitar rechazo.\n",
        "- **Estrategia mixta:** Variar la negociación en base a probabilidades.\n",
        "\n",
        "En este tipo de juegos, la incertidumbre obliga a los jugadores a optimizar sus decisiones en función de posibles respuestas del oponente.\n",
        "\n",
        "\n",
        "\n",
        "## 3. ¿Qué distingue los juegos de suma cero de los juegos de no suma cero y cómo afecta esta diferencia al proceso de toma de decisiones de los jugadores? Proporcione al menos un ejemplo de juegos que entren en la categoría de juegos de no suma cero y discuta las consideraciones estratégicas únicas involucradas\n",
        "\n",
        "### Diferencia entre juegos de suma cero y no suma cero:\n",
        "- **Juegos de suma cero:** La ganancia de un jugador implica la pérdida del otro. Ejemplo: Ajedrez.\n",
        "- **Juegos de no suma cero:** Los jugadores pueden beneficiarse simultáneamente. Ejemplo: Dilema del Prisionero.\n",
        "\n",
        "### **Ejemplo: Dilema del Prisionero**\n",
        "Dos sospechosos pueden **testificar** o **negar** el crimen. La matriz de pagos es:\n",
        "\n",
        "| A / B | Testifica | Niega |\n",
        "|-------|----------|-------|\n",
        "| **Testifica** | (-5, -5) | (-10, 0) |\n",
        "| **Niega** | (0, -10) | (-1, -1) |\n",
        "\n",
        "Las estrategias deben considerar:\n",
        "- La cooperación podría ser mejor para ambos (negar).\n",
        "- En ausencia de comunicación, el equilibrio de Nash es que ambos testifiquen.\n",
        "\n",
        "En juegos de no suma cero, la cooperación y la confianza pueden ser clave para maximizar beneficios.\n",
        "\n",
        "\n",
        "\n",
        "## 4. ¿Cómo se aplica el concepto de equilibrio de Nash a los juegos simultáneos? Explicar cómo el equilibrio de Nash representa una solución estable en la que ningún jugador tiene un incentivo para desviarse unilateralmente de la estrategia elegida\n",
        "\n",
        "El **equilibrio de Nash** es una situación en la que ningún jugador mejora cambiando unilateralmente su estrategia. Matemáticamente, si dos jugadores siguen estrategias \\( \\pi_A^* \\) y \\( \\pi_B^* \\), se cumple:\n",
        "\n",
        "$$ V_A(\\pi_A^*, \\pi_B^*) \\geq V_A(\\pi_A, \\pi_B^*) $$\n",
        "$$ V_B(\\pi_A^*, \\pi_B^*) \\geq V_B(\\pi_A^*, \\pi_B) $$\n",
        "\n",
        "Esto significa que, dada la estrategia del oponente, no hay incentivos para cambiar la propia.\n",
        "\n",
        "### **Ejemplo: Dilema del Prisionero**\n",
        "Si ambos testifican, aunque el resultado no sea óptimo, cambiar de estrategia (negar) resultaría en una peor situación para cada jugador. Por lo tanto, **testificar es el equilibrio de Nash**.\n",
        "\n",
        "Este concepto es fundamental en economía, negociación y teoría de juegos, ya que permite encontrar estrategias estables en situaciones de competencia.\n",
        "\n",
        "\n",
        "\n",
        "## 5. Discuta la aplicación del temporal difference learning en el modelado y optimización de procesos de toma de decisiones en entornos dinámicos. ¿Cómo maneja el temporal difference learning el equilibrio entre exploración y explotación y cuáles son algunos de los desafíos asociados con su implementación en la práctica?\n",
        "\n",
        "TD Learning se aplica en problemas dinámicos donde el entorno cambia y la optimización es clave. Un ejemplo destacado es **Backgammon**, donde un agente ajusta los pesos de evaluación jugando contra sí mismo.\n",
        "\n",
        "### **Exploración vs. Explotación**\n",
        "Para equilibrar la exploración y la explotación, TD Learning utiliza estrategias como:\n",
        "- **\\(\\epsilon\\)-greedy:** Con probabilidad \\( \\epsilon \\) se elige una acción aleatoria para explorar.\n",
        "- **Reducción progresiva de \\( \\epsilon \\):** Se inicia explorando más y se reduce la exploración con el tiempo.\n",
        "\n",
        "### **Desafíos en su implementación**\n",
        "1. **Convergencia lenta:** Aprender valores óptimos en entornos complejos puede ser costoso en términos de tiempo.\n",
        "2. **Dependencia de hiperparámetros:** Factores como la tasa de aprendizaje (\\( \\eta \\)) y el descuento (\\( \\gamma \\)) afectan la estabilidad y el rendimiento.\n",
        "3. **Exploración insuficiente:** Si el agente no explora lo suficiente, puede quedar atrapado en soluciones subóptimas.\n",
        "\n",
        "A pesar de estos retos, TD Learning es una herramienta poderosa en inteligencia artificial, especialmente en juegos y toma de decisiones automatizada.\n"
      ],
      "metadata": {
        "id": "rvwuOwfy06KL"
      }
    }
  ]
}
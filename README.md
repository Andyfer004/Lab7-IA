# Conecta 4 - Entrenamiento con Q-Learning vs Minimax

Este proyecto implementa una versiÃ³n del clÃ¡sico juego **Connect Four** (Conecta 4) en Python, integrando distintos enfoques de inteligencia artificial para simular y comparar el rendimiento entre estrategias de **aprendizaje por refuerzo (Q-Learning)** y **bÃºsqueda adversarial (Minimax y Minimax con poda Alpha-Beta)**.

---

## ğŸ¯ Objetivo

Evaluar el rendimiento de un agente que aprende mediante **Q-Learning** (una tÃ©cnica de **aprendizaje por diferencias temporales**) frente a oponentes que aplican algoritmos determinÃ­sticos como **Minimax** y **Minimax con poda Alpha-Beta**, en una serie de partidas simuladas.

---

## ğŸ“¦ Estructura del Proyecto

- `ConnectFour`: clase que gestiona el estado del tablero y verifica condiciones de victoria.
- `QLearningAgent`: agente que aprende a jugar mediante Q-Learning con una polÃ­tica epsilon-greedy.
- `train_td`: funciÃ³n de entrenamiento para el agente Q-Learning.
- `minimax` y `minimax_alphabeta`: funciones de bÃºsqueda de adversarios.
- `jugar_partido`: simula enfrentamientos entre agentes.
- `matplotlib`: utilizado para graficar los resultados de los enfrentamientos.

---

## ğŸ¤– Entrenamiento

El agente TD se entrena jugando contra sÃ­ mismo durante una cantidad fija de partidas (por defecto 5,000). Durante estas partidas:

- Explora distintas acciones usando una polÃ­tica epsilon-greedy.
- Actualiza su tabla Q en cada turno.
- Aprende a identificar secuencias que lo llevan a ganar o evitar perder.

---

## âš”ï¸ Enfrentamientos

Se jugaron dos series de **50 partidas**:

- **Agente TD vs Minimax**
- **Agente TD vs Minimax con poda Alpha-Beta**

Se registrÃ³ cuÃ¡ntas partidas ganÃ³ cada tipo de agente.

---

## ğŸ“Š Resultados

Los resultados se muestran grÃ¡ficamente con barras que comparan el nÃºmero de victorias del agente TD frente a sus oponentes estratÃ©gicos.

---

## ğŸ“Œ Conclusiones

- El agente entrenado con TD Learning **no superÃ³** a los algoritmos de bÃºsqueda.
- Minimax y Alpha-Beta dominan desde el principio porque exploran el Ã¡rbol de juego y evalÃºan mejores jugadas.
- El agente TD depende completamente del nÃºmero de partidas jugadas, los hiperparÃ¡metros y la calidad de la funciÃ³n de recompensa.

### ğŸ§  Â¿QuÃ© mejorar?
- Aumentar el nÃºmero de episodios de entrenamiento.
- Usar redes neuronales (Deep Q-Learning) en lugar de tablas Q.
- Reducir gradualmente el valor de `epsilon` para explorar menos con el tiempo.
- Aplicar funciones de evaluaciÃ³n mÃ¡s sofisticadas.

---

## â–¶ï¸ Requisitos

```bash
pip install numpy matplotlib json-c
```
ConclusiÃ³n General
Este laboratorio 7 demuestra cÃ³mo la inteligencia artificial puede aplicarse a juegos clÃ¡sicos como Conecta 4, permitiendo comparar enfoques basados en aprendizaje (Q-Learning) con estrategias deterministas (Minimax y Alpha-Beta). A travÃ©s del entrenamiento y simulaciÃ³n de partidas, se evidencia que los mÃ©todos de bÃºsqueda todavÃ­a tienen una ventaja clara frente a agentes que aprenden desde cero, especialmente cuando se usan tablas Q discretas y pocas partidas de entrenamiento.

Sin embargo, tambiÃ©n se ilustra el potencial del aprendizaje por refuerzo para mejorar progresivamente mediante la experiencia. Con ajustes adecuados en los hiperparÃ¡metros, mÃ¡s entrenamiento y tÃ©cnicas mÃ¡s modernas como Deep Q-Learning, el agente puede aspirar a competir en condiciones mÃ¡s justas contra algoritmos clÃ¡sicos.

